{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DeepSphere]: a spherical convolutional neural network\n",
    "[DeepSphere]: https://github.com/SwissDataScienceCenter/DeepSphere\n",
    "\n",
    "[Nathanaël Perraudin](https://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak, Raphael Sgier\n",
    "\n",
    "# Demo: whole sphere classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Run on CPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepsphere import models, experiment_helper, plot\n",
    "from deepsphere.data import LabeledDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'whole_sphere'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data loading\n",
    "\n",
    "The data consists of a toy dataset that is sufficiently small to have fun with. It is made of 200 maps of size `NSIDE=64` splitted into 2 classes. \n",
    "\n",
    "The maps contain a Gaussian random field realisations produced with Synfast function from Healpy package.\n",
    "The input power spectra were taken from LambdaCDM model with two sets of parameters.\n",
    "These maps are not realistic cosmological mass maps, just a toy dataset.\n",
    "We downsampled them to `Nside=64` in order to make the processing faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/maps_downsampled_64.npz')\n",
    "assert(len(data['class1']) == len(data['class2']))\n",
    "nclass = len(data['class1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a map of each class. It is not simple to visually catch the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = min(np.min(data['class1']), np.min(data['class2']))\n",
    "cmax = max(np.max(data['class1']), np.max(data['class2']))\n",
    "cm = plt.cm.RdBu_r\n",
    "cm.set_under('w')\n",
    "hp.mollview(data['class1'][0], title='class 1', nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "hp.mollview(data['class2'][0], title='class 2', nest=True, cmap=cm, min=cmin, max=cmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, those maps have different Power Spectral Densities PSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_psd_class1 = np.empty((nclass, 192))\n",
    "sample_psd_class2 = np.empty((nclass, 192))\n",
    "\n",
    "for i in range(nclass):\n",
    "    sample_psd_class1[i] = experiment_helper.psd(data['class1'][i])\n",
    "    sample_psd_class2[i] = experiment_helper.psd(data['class2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.arange(sample_psd_class1.shape[1])\n",
    "plot.plot_with_std(ell, sample_psd_class1*ell*(ell+1), label='class 1, Omega_matter=0.3, mean', color='b')\n",
    "plot.plot_with_std(ell,sample_psd_class2*ell*(ell+1), label='class 2, Omega_matter=0.5, mean', color='r')\n",
    "plt.legend(fontsize=16);\n",
    "plt.xlim([10, np.max(ell)])\n",
    "plt.ylim([1e-6, 1e-3])\n",
    "# plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\ell$: spherical harmonic index', fontsize=18)\n",
    "plt.ylabel('$C_\\ell \\cdot \\ell \\cdot (\\ell+1)$', fontsize=18)\n",
    "plt.title('Power Spectrum Density, 3-arcmin smoothing, noiseless, Nside=1024', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "Let us split the data into training and testing sets. The raw data is stored into `x_raw` and the power spectrum densities into `x_psd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = np.vstack((data['class1'], data['class2']))\n",
    "x_raw = x_raw / np.mean(x_raw**2) # Apply some normalization (We do not want to affect the mean)\n",
    "x_psd = preprocessing.scale(np.vstack((sample_psd_class1, sample_psd_class2)))\n",
    "\n",
    "# Create the label vector\n",
    "labels = np.zeros([x_raw.shape[0]], dtype=int)\n",
    "labels[nclass:] = 1\n",
    "\n",
    "# Random train / test split\n",
    "ntrain = 150\n",
    "ret = train_test_split(x_raw, x_psd, labels, test_size=2*nclass-ntrain, shuffle=True)\n",
    "x_raw_train, x_raw_test, x_psd_train, x_psd_test, labels_train, labels_test = ret\n",
    "\n",
    "print('Class 1 VS class 2')\n",
    "print('  Training set: {} / {}'.format(np.sum(labels_train==0), np.sum(labels_train==1)))\n",
    "print('  Test set: {} / {}'.format(np.sum(labels_test==0), np.sum(labels_test==1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Classification using SVM\n",
    "\n",
    "As a baseline, let us classify our data using an SVM classifier.\n",
    "\n",
    "* An SVM based on the raw feature cannot discriminate the data because the dimensionality of the data is too large.\n",
    "* We however observe that the PSD features are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(x_raw_train, labels_train) \n",
    "\n",
    "e_train = experiment_helper.model_error(clf, x_raw_train, labels_train)\n",
    "e_test = experiment_helper.model_error(clf, x_raw_test, labels_test)\n",
    "print('The training error is: {}%'.format(e_train*100))\n",
    "print('The testing error is: {}%'.format(e_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_psd_train, labels_train) \n",
    "\n",
    "e_train = experiment_helper.model_error(clf, x_psd_train, labels_train)\n",
    "e_test = experiment_helper.model_error(clf, x_psd_test, labels_test)\n",
    "print('The training error is: {}%'.format(e_train*100))\n",
    "print('The testing error is: {}%'.format(e_test*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Classification using DeepSphere\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network.\n",
    "\n",
    "Three types of architectures are suitable for this task:\n",
    "1. Classic CNN: the classic ConvNet composed of some convolutional layers followed by some fully connected layers.\n",
    "2. Stat layer: a statistical layer, which computes some statistics over the pixels, is inserted between the convolutional and fully connected layers. The role of this added layer is make the prediction invariant to the position of the pixels on the sphere.\n",
    "3. Fully convolutional: the fully connected layers are removed and the network outputs many predictions at various spatial locations that are then averaged.\n",
    "\n",
    "On this simple task, all architectures can reach 100% test accuracy. Nevertheless, the number of parameters to learn decreases and training converges faster. A fully convolutional network is much faster and efficient in terms of parameters. It does however assume that all pixels have the same importance and that their location does not matter. While that is true for cosmological applications, it may not for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dir_name'] = EXP_NAME\n",
    "\n",
    "# Types of layers.\n",
    "params['conv'] = 'chebyshev5'  # Graph convolution: chebyshev5 or monomials.\n",
    "params['pool'] = 'max'  # Pooling: max or average.\n",
    "params['activation'] = 'relu'  # Non-linearity: relu, elu, leaky_relu, softmax, tanh, etc.\n",
    "params['statistics'] = None  # Statistics (for invariance): None, mean, var, meanvar, hist.\n",
    "\n",
    "# Architecture.\n",
    "architecture = 'fully_convolutional'\n",
    "\n",
    "if architecture == 'classic_cnn':\n",
    "    params['statistics'] = None\n",
    "    params['nsides'] = [64, 32, 16, 16]  # Pooling: number of pixels per layer.\n",
    "    params['F'] = [5, 5, 5]  # Graph convolutional layers: number of feature maps.\n",
    "    params['M'] = [50, 2]  # Fully connected layers: output dimensionalities.\n",
    "\n",
    "elif architecture == 'stat_layer':\n",
    "    params['statistics'] = 'meanvar'\n",
    "    params['nsides'] = [64, 32, 16, 16]  # Pooling: number of pixels per layer.\n",
    "    params['F'] = [5, 5, 5]  # Graph convolutional layers: number of feature maps.\n",
    "    params['M'] = [50, 2]  # Fully connected layers: output dimensionalities.\n",
    "\n",
    "elif architecture == 'fully_convolutional':\n",
    "    params['statistics'] = 'mean'\n",
    "    params['nsides'] = [64, 32, 16, 8, 8]\n",
    "    params['F'] = [5, 5, 5, 2]\n",
    "    params['M'] = []\n",
    "\n",
    "params['K'] = [10] * len(params['F'])  # Polynomial orders.\n",
    "params['batch_norm'] = [True] * len(params['F'])  # Batch normalization.\n",
    "\n",
    "# Regularization.\n",
    "params['regularization'] = 0  # Amount of L2 regularization over the weights (will be divided by the number of weights).\n",
    "params['dropout'] = 0.5  # Percentage of neurons to keep.\n",
    "\n",
    "# Training.\n",
    "params['num_epochs'] = 12  # Number of passes through the training data.\n",
    "params['batch_size'] = 16  # Number of samples per training batch. Should be a power of 2 for greater speed.\n",
    "params['eval_frequency'] = 15  # Frequency of model evaluations during training (influence training time).\n",
    "params['scheduler'] = lambda step: 1e-1  # Constant learning rate.\n",
    "params['optimizer'] = lambda lr: tf.train.GradientDescentOptimizer(lr)\n",
    "#params['optimizer'] = lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5)\n",
    "#params['optimizer'] = lambda lr: tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.deepsphere(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup before running again.\n",
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDataset(x_raw_train, labels_train)\n",
    "testing = LabeledDataset(x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_validation, loss_validation, loss_training, t_step = model.fit(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_loss(loss_training, loss_validation, t_step, params['eval_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = experiment_helper.model_error(model, x_raw_train, labels_train)\n",
    "error_test = experiment_helper.model_error(model, x_raw_test, labels_test)\n",
    "print('The training error is: {:.2%}'.format(error_train))\n",
    "print('The testing error is: {:.2%}'.format(error_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Filters visualization\n",
    "\n",
    "The package offers a few different visualizations for the learned filters. First we can simply look at the Chebyshef coefficients. This visualization is not very interpretable for human, but can help for debugging problems related to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=2\n",
    "model.plot_chebyshev_coeffs(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the Chebyshef polynomial, i.e the filters in the graph spectral domain. This visuallization can help to understand wich graph frequencies are picked by the filtering operation. It mostly interpretable by the people for the graph signal processing community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_filters_spectral(layer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes one of the most human friendly representation of the filters. It consists the section of the filters \"projected\" on the sphere. Because of the irregularity of the healpix sampling, this representation of the filters may not look very smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update({'font.size': 16})\n",
    "model.plot_filters_section(layer, title='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we can simply look at the filters on sphere. This representation clearly displays the sampling artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "model.plot_filters_gnomonic(layer, title='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
