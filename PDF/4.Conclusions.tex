
%*******************************************************************************
%*********************************** Conclusions ***************************
%*******************************************************************************
%!TEX root = 0.main.tex

\section{Conclusions}\label{sec:Chapter4}




\subsection{Experimental validation: SHREC17}
\label{sec:Chapter5:Experimental validation}
Gusset \cite{Gusset} implemented the graph proposed in Chapter 2 in a GCNN and three other rotation equivariant neural networks on a popular classification problem \cite{SHREC17}. The four models tested were the following: the original version of DeepSphere, Deepsphere \textit{Optimal} - obtained implementing the thresholding procedure described in this work in section \ref{sec:Chapter2:How to build a good graph} - and the traditional SCNNs of Cohen et al. and of Esteves et al.  \cite{SCNN} \cite{Esteves}.
\paragraph{On the Equiangular sampling.} Gusset compares with two different metrics  (accuracy, F1-score) the performances of these four rotation invariant models, while also comparing the speed of inference and of training of each model. Results are shown in table \ref{tab:SHREC17_class}.  It can be seen how \textit{DeepSphere Optimal} has \textit{always} the highest score between all the rotation equivariant models, no matter the evaluation metric. Furthermore, its performances in terms of speed of inference and training are second only to DeepSphere, remaining by far faster than the other two SCNNs. 
\begin{table}[ht]
	\centering
	\begin{tabular}{l|c c r r r}
		\multicolumn{1}{l}{} & \multicolumn{2}{c}{performance} & \multicolumn{1}{c}{size} & \multicolumn{2}{c}{speed}\\
		\cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6}
		\multicolumn{1}{l}{Method} & Accuracy & F1-score & params & inference & training \\ \hline
		Cohen \emph{s2cnn\_simple} & 78.59 & 78.85 & 400k & 12ms & 32h\\
		Esteves \emph{sphericalcnn} & 79.18 & 79.36 & 500k & 9.8ms & 2h52\\ \hline
		Deepsphere & 73.36 & 73.67 & 190k & \textbf{0.98ms} & \textbf{43m} \\
		\textbf{Deepsphere \emph{Optimal}} & \textbf{80.42} & \textbf{80.65} & 190k & 1.0ms & 48m
	\end{tabular}
	\caption{Results form \cite{Gusset}. Performances of four rotation equivariant GCNNs and two SCNNs on the popular classification task SHREC17.}
	\label{tab:SHREC17_class}
\end{table}
\paragraph{On HEALPix }
Gusset repeated the same test on the same dataset, this time sampled using the HEALPix sampling scheme with $N_{side}=32$. Results can be seen in table \ref{table:results}.
\begin{table}[h!]
	\centering
	\begin{tabular}{ c|c|c } 
		& DeepSphere & DeepSphere \textit{Optimal} \\ 
		\hline
		accuracy & 82.23\% & 82.76\% \\ 
	\end{tabular}
	\caption{\label{table:results}Results form Gusset et al. Accuracy on the HEALPix sampling}
\end{table}
Being the new graph of DeepSphere Optimal more equivariant to rotations, we expected to see an improvement in the accuracy, as we did in the equiangular case. The fact that this improvement was not observed means that, with this sampling, the original DeepSphere graph $W$ \textit{is already sufficiently equivariant to rotations}.

\subsection{Confront of different Discrete Laplacians on the equiangular sampling}
We conclude by showing how the different discrete Laplacians $\mathbf L$ illustrated so far compare in terms of equivariance error and computational time of the filter $\mathcal F(\mathbf f) = \mathbf L\mathbf f$. We can see how the four sparse discrete Laplacians are one order of magnitude faster than the two full Laplacians. The FEM Laplacian is able to reduce the equivariance error of the HKGL, and it manages to keep it low - around $0.5\%$ - even when using the sparse, lumped approximation $\mathbf D^{-1}\mathbf A$ while reducing the computational time of one order of magnitude. $\mathbf D^{-1}\mathbf A$ performs really well, and gets close to the performances of the graph Laplacian of Khasanova and Frossard. 
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{../codes/06.Equivariance_error/tradeoff.png}
	\caption{\label{fig:tradeoff}Trade-off between computational time and equivariance error for the filter $\mathbf L$ for different discrete Laplacians on the equiangular sampling}
\end{figure}

\begin{table}[h!]
	\centering
	\footnotesize
	\begin{tabular}{ c|c |c|c|c|c|c } 
		& FEM & HKGL & Lumped & Symmetric & Thresholded & Khasanova  \\
		&&&FEM&Lumped&HKGL&Frossard\\
		&&&& FEM&&\\
		\hline
		Equivariance & 0.46 & 1.55 & 0.47 & 2.80& 1.50 & 0.64\\
		error [\%]&&&&&&\\ \hline
		Computational & 619 & 561 & 68 & 49 & 67 & 42 \\
		 time [$\mu$s]&&&&&&
	\end{tabular}
	\caption{\label{table:res}Results of figure \ref{fig:tradeoff}}
\end{table}

\subsection{Final considerations and future work}
In order not to confuse the notation between the FEM and the Graph approach we will be need a more precise notation than in the rest of this work. For this purpose, define a graph $G$, and its graph Laplacian by $L_G$. Define $\mathbf V_G,\ \mathbf \Lambda_G$ to be the solution of the eigenvalue problem
$$
\mathbf L_G \mathbf V_G = \mathbf V_G\mathbf  \Lambda_G.
$$
Define the FEM stiffness matrix $\mathbf A,\ (\mathbf A)_{ij} = \int\nabla\phi_i\nabla\phi_j$ and the FEM mass matrix $\mathbf B,\ (\mathbf B)_{ij} = \int\phi_i\phi_j$. Define $\mathbf V_{FEM}$ and $\mathbf \Lambda_{FEM}$ to be the solution to the generalized eigenvalue problem 
$$
\mathbf A\mathbf V_{FEM} =\mathbf  B\mathbf V_{FEM}\mathbf \Lambda_{FEM}.
$$
We saw that both in the graph and in the FEM approach, filtering a sampled signal means approximating the Fourier transform through the multiplication of the signal $\mathbf f$ by a Fourier matrix - $\mathbf V_G^\intercal$ for the graph, $\mathbf V_{FEM}^\intercal\mathbf B$ for the FEM - then applying a filter through a diagonal matrix $\mathbf K$, and then applying the inverse Fourier transform - $\mathbf V_G$ for the graph, $(\mathbf V_{FEM}^\intercal\mathbf B)^{-1}$ for the FEM -. From these considerations it follows that a polynomial filter $P_\kappa(\mathbf \Lambda)$ is implemented in the graph domain by multiplying the signal $\mathbf f$ by a polynomial of the symmetric graph Laplacian $\mathbf L_G$
$$
P_\kappa(\mathbf L_G),
$$
and in the FEM domain (thanks to what explained in section \ref{sec:FEM filtering as a graph filtering}) by a polynomial of the matrix $\mathbf B^{-1}\mathbf A$
$$P_\kappa(\mathbf B^{-1}\mathbf A).
$$

Starting from the FEM Laplacian $\mathbf B^{-1}\mathbf A$ it is possible to construct a \textit{sparse} Laplacian $\mathbf D^{-1}\mathbf A$ that shows almost no difference in its equivariance error compared to the full FEM Laplacian. Levy \cite{levy} showed that, by explicitly solving the integrals $\int_\tau \phi_i \phi_j$, $\int_\tau \nabla \phi_i \cdot \nabla \phi_j$, 
$$\begin{aligned}
\mathbf D_{ii} &= \frac{A_i}{3},\\
\mathbf A_{ij} &= \frac{1}{2}\left(\text{cot}(\alpha_{ij}) + \text{cot}(\beta_{ij})\right)
\end{aligned}$$
where $A_i$ is the sum of the area of all the triangles $\tau$ of the triangulation $\mathcal T_h$ sharing the $i$th vertex, proving that the lumped FEM Laplacian $\mathbf D^{-1}\mathbf A$ corresponds to the Laplacian of Desbrun et al. \cite{Desbrun1999} introduced in this work in section \ref{sec:Chapter3: other discrete laplacians}. In this way he connected the FEM approach to Laplacians obtained from Differential Geometry and Discrete Exterior Calculus \cite{vallet}, \cite{meyer}.

\begin{snugshade*}
	The  symmetric  graph Laplacian $\mathbf L_G$ constrains the graph Fourier matrix $\mathbf V_G^\intercal$ to be orthogonal, while the FEM Laplacian $\mathbf B^{-1}\mathbf A$ leaves to its Fourier matrix $\mathbf V_{FEM}^\intercal\mathbf B$ more degrees of freedom. The fact that the mass matrix $\mathbf B$ is constructed to \textit{exactly} represent the dot product in the Galerkin subspace $V_h$ and that the matrix $\mathbf V_{FEM}$ converges to the sampled spherical harmonics makes it possible for the FEM filtering to converge towards the continuous filtering even in cases of non uniform sampling measures \cite{Quarteroni:1639539}, while for symmetric Laplacians there is no convergence result available.
\end{snugshade*}

How to interpret the fact that the graph approach constrains the Fourier matrix $\mathbf V_G^\intercal$ to be orthogonal is still not clear and will be subject of future work. However, even given these orthogonality constraints it is sometimes possible to design graphs with state-of-the-art performances, like the Khasanova-Frossard graph for the equiangular sampling scheme. Notice that this graph was obtained solving the optimization problem (\ref{eq:minimization frossard}) formulated directly in the spatial (vertex) domain, without relying on the spectral interpretation of the graph filtering, that in the case of a non uniform sampling presents the problems discussed above and it is still not clear.

