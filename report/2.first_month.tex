%*******************************************************************************
%*********************************** Second Chapter *****************************
%*******************************************************************************
%!TEX root = 0.main.tex
\section{First Month}
\subsection{Daniel Spielman's notes, Jupyter Notebook "Experience1" and meeting of Tuesday 26 February}

One interesting thing:

\begin{theorem}{\textit{Lemma 5.2.1 of Daniel Spielman's Lecture Notes of the course of Spectral Graph Theory (2015)}}
	\label{theo:eigenvectors on the ring}
	The Laplacian of the 2-Nearest-Neighbours ring graph $R_n$ on $n$ equispaced vertices has eigenvectors
	$$\mathbf x_k(u) = \cos(2k\pi u/n)$$
	$$\mathbf y_k(u) = \sin(2k\pi u/n)$$
	for $0\leq k\leq n/2$, ignoring $\mathbf y_0 = \mathbf 0$ and for even $n$ ignoring $\mathbf y_{n/2}$ for the same reason. Eigenvectors $\mathbf x_k, \mathbf y_k$ have eigenvalues $2-2\cos(2\pi k/n)$
\end{theorem}

This lemma make us realize the fact that for a Graph Laplacian it is possible to have eigenvectors equal to the spherical harmonics sampled in the nodes, but has completely different eigenvalues! Furthermore, if I artificially build a matrix $L'=U\Lambda'U^T$ such that $L=U\Lambda U^T$ but with $\Lambda'=\text{diag}\{-1, -4, -4, -9, -9, ...\}$ the eigenvalues of the operator  
$$\triangle \Phi(\rho, \theta) = \frac{1}{\rho}\partial_\rho \left(\frac{1}{\rho}\partial_\rho \Phi\right) + \frac{1}{\rho^2}\partial_{\theta\theta}\Phi$$ 
Laplace-Beltrami on the circle  So, this tells us that the problem proposed by Micha\"el
\begin{equation}\label{eq:michaelsproblem}
	W^\star = \arg\min_W||U-\hat U||, \quad L=D-W,\quad LU=U\Lambda
	 \hat U = \text{"Spherical harmonics sampled in the point set P"}
\end{equation}

could be badly posed, in the sens that once found the weights $W^\star$ that give us the $U^\star$ realizing the minimum above, we have no clue of which eigenvalues to set in order to retrieve a true Graph Laplacian (positive diagonal, negative entries, symmetric, row/column sum equal to $0$).\\
Nathana\"el proposes a different approach. After having observed that given a non-uniform sampling (that's the setting we are aiming at) the vectors of the spherical harmonics sampled in the point set $P$ won't be orthogonal, and thus a spectral decomposition of the Laplacian will never realize Micha\"el's problem \ref{eq:michaelsproblem} he proposes the following problem:
\begin{equation}\label{eq:nathanaelsproblem}
	\arg\min_W||L\hat U-\hat U\hat \Lambda||_\mathcal F,\quad L=D-W,\quad  W>0,\quad D=\text{diag}\{\sum_{i}(W)_{i,j}\}
\end{equation}

where $\hat{\Lambda}, \hat{U}$ are respectively the eigenvalues and eigenvectors of $\triangle_\mathcal M$ the operator of Laplace-Beltrami sampled in the point set P. In this way we get the graph laplacian that has the closest spectral decomposition to the "true" one. The problem \ref{eq:nathanaelsproblem} can be rewritten in the following form 

\begin{equation}\label{eq:nathanaelsproblem2}
[w^\star\ \lambda^\star]^T = \arg\min_W||A[w\ \lambda]^T||_2^2
\end{equation}

However, Micha\"el is not convinced by the fact that the form of the problem seems redundant: $\lambda=\lambda(w)$ is a function of $w$. So, the optimization is done on both $\lambda, w$ that can't respect the relationship $\lambda=\lambda(w)$. At the end only one of the two arguments of the minimization problem $[w^\star\ \lambda^\star]^T$ will be used to obtain the second according to the relation $\lambda=\lambda(w)$.

Waiting for Nathan\"el's notes to make this passage clearer


\subsection{Nath's problem}

Given a matrix $U$, we would like to find a valid graph such that
its eigenbase is as close to $U$ as possible. Note that $U$ might
contain only a few eigenvectors (be rectangular) and might not be
orthogonal, i.e.: $UU^{*}\neq I$.

\subsection*{Some remarks}
\begin{itemize}
	\item The Fourier basis (DFT) is probably only available for a circulant
	matrix
	\item If $U$ is not orthogonal, it is useless to try to use it directly
	to build the Laplacian
	\item The value of the eigenvalues $\Lambda$ cannot be known in advance
	\item Learning a graph given an eigenbasis has been done \cite{pasdeloup2018characterization,shafipour2017network}
	It does not work well according to me and I do not like the papers.
	\item For graph learning, the best paper to read is probably \cite{kalofolias2016learn}.
	Some more advanced results are availlable in \cite{kalofolias2018large}.
	I might be biased in recommending these two papers. You can check
	the references inside if you want to see more.
\end{itemize}

\subsection*{Problem formulation}

Solving the problem directly, i.e. searching for $U$ and $\Lambda$
is, according to me, too complicated. Hence I propose an indirect
solution. Instead of searching for $U,\Lambda$ such that $L=U\Lambda U^{*}$,
we search for $L$ such that:

\[
LU=\Lambda U
\]
We formulate the optimization problem as:

\[
\text{arg}\min_{w}\|LU-\Lambda U\|_{F}^{2}\hspace{1em}\text{s.t.}\lambda^{*}\mathbf{1}=1,w\geq0,\lambda\ge0
\]
Here $w\geq0,\lambda\geq0$ ensure that we have a valid graph and
$\lambda^{*}\mathbf{1}=1$ ensures that the trivial solution $w=0$
is not selected. Now $L$ is linearly linked to $W$ by
\[
L_{ij}=\begin{cases}
-W_{ij} & \text{if }i\ne j\\
\sum_{i}W_{ij} & \text{otherwise.}
\end{cases}
\]
Let us write this operator 
\[
\ell=O_{L}w=\text{vec}\left(\text{diag}(\text{mat}(w)\mathbf{1})-\text{mat}(w)\right)
\]
For simplicity, we assume $W_{i,i}=0$. Furthermore $LU$ is also
a linear operation. Let us write the operator $O_{U}.$ We have 
\begin{align*}
\left(LU\right)_{o,p} & =\sum_{j}L_{o,j}U_{j,p}\\
& =U_{o,p}\sum_{i}W_{io}-\sum_{j}W_{o,j}U_{j,p}
\end{align*}
Similarly, we have we define $O_{\Lambda}$ such that 
\[
O_{\Lambda}\lambda=\text{vec}(\text{diag}(\lambda)U).
\]
Eventually, we have 
\[
\|LU-\Lambda U\|_{F}^{2}=\left\Vert A\left[\begin{array}{c}
w\\
\lambda
\end{array}\right]\right\Vert _{2}^{2}
\]
where $A=\left[\begin{array}{cc}
O_{U}O_{L} & O_{\Lambda}\end{array}\right]$. The problem can be solved with a quadratic programing (QP) solver.

\subsection*{Alternative problem formulation}

The previous problem formulation penalizes mostly the spectral modes associated with high eigenvalues. In an attempt to solve this issue, we can formate the quadratic term as

\[
\|\Lambda^{-1}LU-U\|_{F}^{2}=\|\Lambda^{-1}(D-W)U-U\|_{F}^{2}.
\]
This implies that the variables $w,\lambda$ are mixed and the problem
is no longer convex. Let $T=\Lambda^{-1}(D-W)=\Lambda^{-1}D-\Lambda^{-1}W,$
then 
\[
T_{ij}=\begin{cases}
-\lambda_{i}^{-1}W_{i,j} & \text{if }i\ne j\\
\lambda_{i}^{-1}\sum_{j}W_{i,j} & \text{otherwise.}
\end{cases}
\]
Now if we were to minimize the quadratic $\|TU-U\|_{F}^{2}$, a trivial
solution would be $T=I.$ I do not think, this leads anywhere. 

\subsection*{Other ideas}

Which is the closest $U$ from $U_{G}$ such that it remains orthonormal?
\[
\text{arg}\min_{U}\|U_{G}-U\|_{F}\hspace{1em}UU^{*}=I
\]

\subsection{Weeks 2-3 (26 Feb - 12 Mar)}

\subsubsection*{Nath's problem $\mathbf{\arg \min ||LU-U\Lambda||}$}
I implemented Nathana\"el's problem in the file 04\_optimization.py on a sampled ring, in order to have the possibility to refer to theorem \ref{theo:eigenvectors on the ring} that I correctly implemented in the notebook 02\_Experience1.py. Theorem \ref{theo:eigenvectors on the ring} tells us that in some cases (uniform sampling) it is possible to have as G-eigenvectors the sampled $\mathcal{M}$-eigenvectors, but with different eigenvalues! This means (as we understood with Micha\"el on Tuesday 12 March) that if we want to engineer a graph filter $F_G(f_{(P)}) = Uh(\Lambda_G)U^Tf_{(P)}$ such that is acts on the graph-spectrum of the sampled signal $f(P)$ as the continuous $\mathcal{M}$-filter $F_\mathcal M(f)_{(x)} =  \mathcal F^{-1}\left(\sum_i h_{\mathcal M}(\lambda_{\mathcal M, i}) Y_i(\omega)\right)(x)$ acts on the $\mathcal M$-spectrum of the continuous signal $f(x)$ we need to \textit{identify the graph eigenvalues with the corresponding $\mathcal M$-eigenvalues to make sure that $h_G(\lambda_{i,G}) = h_{\mathcal M}(\lambda_{i,\mathcal M}) $}.\\
We observed that the result of the optimization for uniform sampling and for non-uniform sampling. Results are the in PDM/codes/04\_optimization/04\_figs. We see a good correspondence between $\Lambda^\star$ and the eigenvalues of the optimal Laplacian $L^\star$. However, we do not observe a good correspondence between the eigenvectors and the sampled spherical harmonics. However, it is really hard to interpret the scalar product between the vectors of the sampled spherical harmonics and the eigenvectors when the sampling is not uniform!\\
\textbf{TO DO}: to compare the graph eigenvectors with the sampled spherical harmonics we did a scalar product of the vectors. However, we need to do a proper NUDFT (Non Uniform Discrete Fourier Transform https://en.wikipedia.org/wiki/Non-uniform\_discrete\_Fourier\_transform), that is a proper discretization of the integral $\int_{\mathcal S_1} u_i(x) Y_j(x) dx$ where $u(x)$ is the continuous graph eigenvector (\textbf{QUESTION}: what does this mean?) and $Y_j$ is the $j$-th spherical harmonic. The scalar product in $\mathbb R^N$ of the sampled signals corresponds to the scalar product in $L_2(\mathcal S_1)$ and thus to a DFT only for band-limited signals and in case of uniform sampling, since the vectors of the sampled harmonics are orthonormal!
\subsubsection*{G-orthogonality and $\mathcal M$-orthogonality}
It is key to distinguish between the concepts of G-orthogonality and $\mathcal M$-orthogonality. This generated a lot of confusion in the past weeks. The question is the following: what is the SHT of a signal? The answer depends on the functional setting of the class of signals we want to work with. If we define the class of signals 

$$\mathcal F = \{f: \mathcal M \rightarrow \mathbb R: SHT(f)_i = 0\quad \forall i \geq N \}$$ 

and the sampling set $P = \{x_i \in \mathcal M\}_{i=0}^N$ we have that 

$$\forall f\in \mathcal F,  \forall x\in \mathcal M \quad \mathbf f(x)=\sum \hat f_i Y_i(x)$$

Where $\mathbf {\hat f} = \{\hat f_i\} = SHT(f)$. Thus on the sampling point set $P$ we can write

$$\forall f\in \mathcal F\quad \mathbf f_{(P)}=\sum \hat f_i Y_{i(P)} = Y_P \mathbf {\hat  f}$$


If the matrix $Y_P$ of the sampled spherical harmonics is full rank, for these signals it is true that

$$\forall f \in \mathcal F \quad SHT(f) = \mathbf{\hat f} = Y_P^{-1} f_{(P)}$$

However, for a general signal on the manifold, the $SHT(f)$ is given by the Type 2 - Non Uniform Discrete Fourier Transform (Type 2 NUDFT). From my understanding, it provides an approximation of the transform since a non-uniform sampling is capable of catching all the frequencies, but once we cut the Fourier series we do not have a Shannon Theorem (perfect reconstruction of band-limited signals).\\
The NUDFT requires to set as a parameter the maximum frequency we want to project our signal on.

On the sphere the NASA provides us with the function \lstinline {anafast} that implements a Type2-NUDFT on the HEALPix sampling out of the box. This program performs harmonic analysis of the HEALPix maps up to a user specified maximum spherical harmonic order $l_{max}$.

\subsubsection*{The first question}
For a signal $f: \mathcal M \rightarrow \mathbb R$ define $\mathbf{\hat f}_G = U^T f(P)$ and  $\mathbf{\hat f}_\mathcal M = SHT(f)$.
The question is the following: is there a graph Fourier basis $U: U^TU=I$ such that $\mathbf{\hat f}_G = \mathbf{\hat f}_\mathcal M \forall f$?

If this is true, I have a graph such that the G-spectrum is equal to the $\mathcal M$-spectrum. However, nothing is said about the eigenspaces. \textbf{In any case, the answer to this question is NO}. 
In the setting of band limited signals $\mathcal F = \{f: \mathcal M \rightarrow \mathbb R: SHT(f)_i = 0\quad \forall i \geq N \}$  if the matrix $Y_P$ is full rank we have that $U$ has to be equal to $Y$ since $\mathbf{\hat f_G} = U^Tf(P)$ and $\mathbf{\hat f_\mathcal M} = Y_P^{-1}f(P)$. Even if we relax the request asking
 
$$(\mathbf{\hat f}_{G})_i = (\mathbf{\hat f}_{\mathcal M})_i \quad\forall i \leq n<N \quad \forall f$$

still this is impossible since from this it must follow that the first $n$ columns of $Y^{-1}_P$ are orthonormal, not true for a general sampling $P$.


\subsubsection*{The second question}
Is it possible to find a graph such that the G-eigenspaces are aligned with the $\mathcal M$-eigenspaces? \textbf{Problem: It is a badly formulated question! It doesn't mean much if we do not specify it better.} The formulation 

\[Y^TU = 
\left[ {\begin{array}{ccccc}
	B_{1\times 1} & 0 & 0 & 0 & ...\\
	0 & B_{3\times 3} & 0 & 0 & ...\\
	0 & 0 & B_{5\times 5} & 0 & ...\\
	0 & 0 & 0 & B_{7\times 7} & ...\\
	... & ... & ... & ... & ...\\
	\end{array} } \right]
\]

is not clear since we are projecting the graph eigenvectors on the sampled spherical harmonics in $\mathbb R^N$, but this corresponds to nothing!! What we want is a concept of "alignment" in the continuous domain: so, a better formulation could be the following:


\[SHT(U) = 
\left[ {\begin{array}{ccccc}
	B_{1\times 1} & 0 & 0 & 0 & ...\\
	0 & B_{3\times 3} & 0 & 0 & ...\\
	0 & 0 & B_{5\times 5} & 0 & ...\\
	0 & 0 & 0 & B_{7\times 7} & ...\\
	... & ... & ... & ... & ...\\
	\end{array} } \right]
\]

But again here we have a problem: when we write $SHT(U)$ we are imagining the eigenvectors $\mathbf u_i\in \mathbb R^N$ as traces of a continuous signal $u_i(x): \mathcal M \rightarrow \mathbb R$ and what we want is actually the $SHT(u_i(x))$. The problem is that on a uniform sampling we are limited by the Shannon theorem, that means we can reconstruct the spectrum of $u_i(x)$ only up to a certain frequency, and on a non-uniform sampling we do not know anything at all! On a non-uniform sampled signal there's no way of reconstructing the spectrum, we can only approximate it with a NUDFT. 
\paragraph{Let's now limit ourself to the usual class of band-limited signals.} We know that

$$f(x) = \sum_{i=0}^N \alpha_i Y_i(x)$$

If we suppose that the graph eigenvectors $\mathbf u_j \in \mathbb R^N$ are the trace on the vertices of a "continuous graph eigenvector" $u_j(x) \in \mathcal F$ then we know that 

$$u_j(x) = \sum_{i=0}^N \alpha^j_i Y_i(x)$$

and 

$$SHT(u_i) = Y_P^{-1}u_j(P) = Y_P^{-1}\mathbf u_j$$

so the question becomes
\begin{equation}
SHT(U) = Y_P^{-1}U = \begin{bmatrix}
		B_{1\times 1} & 0 & 0 & 0 & ...\\
		0 & B_{3\times 3} & 0 & 0 & ...\\
		0 & 0 & B_{5\times 5} & 0 & ...\\
		0 & 0 & 0 & B_{7\times 7} & ...\\
		... & ... & ... & ... & ...
		\end{bmatrix} \label{eq:SHT(U)}
\end{equation}


A procedure of proving that this is impossible by absurd could be the following: multiplying eq \ref{eq:SHT(U)} by $Y_P$ we have that

$$
U = Y_P\begin{bmatrix}
B_{1\times 1} & 0 & 0 & 0 & ...\\
0 & B_{3\times 3} & 0 & 0 & ...\\
0 & 0 & B_{5\times 5} & 0 & ...\\
0 & 0 & 0 & B_{7\times 7} & ...\\
... & ... & ... & ... & ...
\end{bmatrix}
$$
 that rewritten column by columns becomes:

 $u_0 = \alpha_1\mathbf Y_0$\\
 $u_1 = \alpha_1^1 \mathbf Y_1 + \alpha_2^1 \mathbf Y_2+ \alpha_3^1 \mathbf Y_3$\\
 $u_2 = \alpha_1^2 \mathbf Y_1 + \alpha_2^2 \mathbf Y_2+ \alpha_3^2 \mathbf Y_3$\\
 $u_3 = \alpha_1^3 \mathbf Y_1 + \alpha_2^3 \mathbf Y_2+ \alpha_3^3 \mathbf Y_3$\\
 ...\\
 
 and so on. I think that the answer to this question is again a no-no: since we must impose $UU^T=I$, this means that we have to find an orthogonal basis of each eigenspace 
 $\text{span}\{\mathbf Y_0\} = \text{span}\{\mathbf u_0\},  \text{span}\{\mathbf Y_1, \mathbf Y_2, \mathbf Y_3\} = \text{span}\{\mathbf u_1, \mathbf u_2, \mathbf u_3\}, \text{span}\{\mathbf Y_4, \mathbf Y_5, \mathbf Y_6, \mathbf Y_7, \mathbf Y_8\}=\text{span}\{\mathbf u_4, \mathbf u_5, \mathbf u_6, \mathbf u_7, \mathbf u_8\}$, etc. etc. (no problem) but the problem comes in when we must impose the $\mathbb R^N$-orthogonality between the different basis of the eigenspaces! This cannot be assured in general. \textbf{TO DO: if you're not convinced by the impossibility of this, find a counter example just with the first two eigenspaces ($|P|=4 \implies \mathbf u_i, \mathbf Y_i \in \mathbb R^4, i=0,1,2,3$)}
